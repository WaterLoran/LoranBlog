# Locust åˆ†å¸ƒå¼è¿è¡Œè¯¦è§£ä¸å®æˆ˜ç¤ºä¾‹

Locust çš„åˆ†å¸ƒå¼è¿è¡Œæ¨¡å¼å…è®¸ä½ åœ¨å¤šå°æœºå™¨ä¸Šåˆ†å¸ƒè´Ÿè½½æµ‹è¯•ï¼Œä»è€Œç”Ÿæˆæ¯”å•æœºæ›´å¤§çš„å¹¶å‘å‹åŠ›ã€‚ä¸‹é¢è¯¦ç»†ä»‹ç»åˆ†å¸ƒå¼è¿è¡Œçš„åŸç†ã€é…ç½®å’Œå®æˆ˜ç¤ºä¾‹ã€‚

## ğŸ“š åˆ†å¸ƒå¼æ¶æ„æ¦‚è¿°

Locust åˆ†å¸ƒå¼é‡‡ç”¨ **ä¸»ä»æ¶æ„ (Master-Worker)**ï¼š

- **Master èŠ‚ç‚¹**ï¼šåè°ƒæµ‹è¯•ï¼Œæ”¶é›†ç»Ÿè®¡ä¿¡æ¯ï¼Œæä¾› Web UI
- **Worker èŠ‚ç‚¹**ï¼šæ‰§è¡Œå®é™…è´Ÿè½½æµ‹è¯•ï¼Œç”Ÿæˆè™šæ‹Ÿç”¨æˆ·

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Master    â”‚â—„â”€â”€Web UI (8089)
â”‚  èŠ‚ç‚¹       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Worker 1   â”‚ â”‚  Worker 2   â”‚
â”‚  èŠ‚ç‚¹       â”‚ â”‚  èŠ‚ç‚¹       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ åŸºç¡€åˆ†å¸ƒå¼è¿è¡Œç¤ºä¾‹

### 1. å¯åŠ¨ Master èŠ‚ç‚¹

```bash
# åœ¨ Master æœºå™¨ä¸Šæ‰§è¡Œ
locust -f locustfile.py --master --master-bind-host=0.0.0.0 --master-bind-port=5557
```

**å‚æ•°è¯´æ˜ï¼š**
- `--master`ï¼šæŒ‡å®šä¸º Master æ¨¡å¼
- `--master-bind-host`ï¼šMaster ç»‘å®šçš„ä¸»æœºåœ°å€
- `--master-bind-port`ï¼šMaster ç»‘å®šçš„ç«¯å£

### 2. å¯åŠ¨ Worker èŠ‚ç‚¹

```bash
# åœ¨ Worker æœºå™¨ä¸Šæ‰§è¡Œï¼ˆå¯ä»¥å¤šå°ï¼‰
locust -f locustfile.py --worker --master-host=192.168.1.100 --master-port=5557
```

**å‚æ•°è¯´æ˜ï¼š**
- `--worker`ï¼šæŒ‡å®šä¸º Worker æ¨¡å¼
- `--master-host`ï¼šMaster èŠ‚ç‚¹çš„ IP åœ°å€
- `--master-port`ï¼šMaster èŠ‚ç‚¹çš„ç«¯å£

## ğŸ”§ å®Œæ•´çš„åˆ†å¸ƒå¼æµ‹è¯•ç¤ºä¾‹

### Locust æµ‹è¯•è„šæœ¬

```python
# distributed_demo.py
from locust import HttpUser, task, between, events
import time
import json

class WebsiteUser(HttpUser):
    wait_time = between(1, 3)
    host = "http://example.com"
    
    def on_start(self):
        """ç”¨æˆ·å¯åŠ¨æ—¶æ‰§è¡Œ"""
        print(f"ç”¨æˆ·å¯åŠ¨åœ¨ Worker ä¸Š")
        self.login()
    
    def login(self):
        """ç™»å½•æ“ä½œ"""
        response = self.client.post("/login", json={
            "username": "testuser",
            "password": "testpass"
        })
        if response.status_code == 200:
            self.auth_token = response.json().get("token")
            print("ç™»å½•æˆåŠŸ")
    
    @task(3)
    def view_homepage(self):
        """æµè§ˆé¦–é¡µ"""
        with self.client.get("/", catch_response=True, name="é¦–é¡µ") as response:
            if response.status_code == 200:
                response.success()
            else:
                response.failure(f"é¦–é¡µè®¿é—®å¤±è´¥: {response.status_code}")
    
    @task(2)
    def view_products(self):
        """æµè§ˆäº§å“åˆ—è¡¨"""
        self.client.get("/products")
    
    @task(1)
    def purchase_product(self):
        """è´­ä¹°äº§å“"""
        headers = {"Authorization": f"Bearer {getattr(self, 'auth_token', '')}"}
        self.client.post("/purchase", json={"product_id": 1}, headers=headers)

# åˆ†å¸ƒå¼äº‹ä»¶å¤„ç†
class DistributedEventHandler:
    def __init__(self):
        self.worker_count = 0
        
    @events.init.add_listener
    def on_locust_init(environment, **kwargs):
        print(f"èŠ‚ç‚¹åˆå§‹åŒ–: {'Master' if environment.parsed_options.master else 'Worker'}")
        
    @events.test_start.add_listener
    def on_test_start(environment, **kwargs):
        node_type = "Master" if environment.parsed_options.master else "Worker"
        print(f"ğŸš€ æµ‹è¯•åœ¨ {node_type} èŠ‚ç‚¹ä¸Šå¼€å§‹")
        
    @events.test_stop.add_listener
    def on_test_stop(environment, **kwargs):
        node_type = "Master" if environment.parsed_options.master else "Worker"
        print(f"ğŸ›‘ æµ‹è¯•åœ¨ {node_type} èŠ‚ç‚¹ä¸Šåœæ­¢")

# åˆå§‹åŒ–äº‹ä»¶å¤„ç†å™¨
distributed_handler = DistributedEventHandler()
```

### å¯åŠ¨è„šæœ¬ç¤ºä¾‹

#### Windows æ‰¹å¤„ç†è„šæœ¬
```batch
@echo off
REM start_master.bat
echo å¯åŠ¨ Locust Master èŠ‚ç‚¹...
locust -f distributed_demo.py --master --master-bind-host=0.0.0.0 --master-bind-port=5557 --web-host=0.0.0.0 --web-port=8089
pause
```

```batch
@echo off
REM start_worker.bat
set MASTER_HOST=192.168.1.100
echo å¯åŠ¨ Locust Worker èŠ‚ç‚¹ï¼Œè¿æ¥åˆ° Master: %MASTER_HOST%
locust -f distributed_demo.py --worker --master-host=%MASTER_HOST% --master-port=5557
pause
```

#### Linux/Mac Shell è„šæœ¬
```bash
#!/bin/bash
# start_master.sh
echo "å¯åŠ¨ Locust Master èŠ‚ç‚¹..."
locust -f distributed_demo.py --master \
    --master-bind-host=0.0.0.0 \
    --master-bind-port=5557 \
    --web-host=0.0.0.0 \
    --web-port=8089
```

```bash
#!/bin/bash
# start_worker.sh
MASTER_HOST="192.168.1.100"
echo "å¯åŠ¨ Locust Worker èŠ‚ç‚¹ï¼Œè¿æ¥åˆ° Master: $MASTER_HOST"
locust -f distributed_demo.py --worker \
    --master-host=$MASTER_HOST \
    --master-port=5557
```

## ğŸ”„ é«˜çº§åˆ†å¸ƒå¼é…ç½®

### 1. ä½¿ç”¨ Docker Compose è¿è¡Œåˆ†å¸ƒå¼é›†ç¾¤

```yaml
# docker-compose.yml
version: '3'

services:
  master:
    image: locustio/locust
    ports:
      - "8089:8089"
      - "5557:5557"
    volumes:
      - ./:/mnt/locust
    command: >
      -f /mnt/locust/distributed_demo.py
      --master
      --master-bind-host=0.0.0.0
      --master-bind-port=5557
      --web-host=0.0.0.0

  worker:
    image: locustio/locust
    volumes:
      - ./:/mnt/locust
    command: >
      -f /mnt/locust/distributed_demo.py
      --worker
      --master-host=master
    deploy:
      replicas: 4
    depends_on:
      - master
```

å¯åŠ¨å‘½ä»¤ï¼š
```bash
docker-compose up --scale worker=4
```

### 2. åŠ¨æ€ Worker ç®¡ç†

```python
# dynamic_worker_manager.py
import subprocess
import time
from threading import Thread

class DynamicWorkerManager:
    def __init__(self, master_host, master_port, locust_file):
        self.master_host = master_host
        self.master_port = master_port
        self.locust_file = locust_file
        self.worker_processes = []
    
    def start_worker(self, worker_id):
        """å¯åŠ¨ä¸€ä¸ª Worker è¿›ç¨‹"""
        cmd = [
            'locust', 
            '-f', self.locust_file,
            '--worker',
            '--master-host', self.master_host,
            '--master-port', str(self.master_port)
        ]
        
        process = subprocess.Popen(cmd)
        self.worker_processes.append((worker_id, process))
        print(f"âœ… Worker {worker_id} å·²å¯åŠ¨")
        return process
    
    def scale_workers(self, target_count):
        """åŠ¨æ€è°ƒæ•´ Worker æ•°é‡"""
        current_count = len(self.worker_processes)
        
        if target_count > current_count:
            # éœ€è¦å¯åŠ¨æ›´å¤š Worker
            for i in range(current_count, target_count):
                self.start_worker(i)
                time.sleep(1)  # é¿å…åŒæ—¶å¯åŠ¨é€ æˆå†²å‡»
        elif target_count < current_count:
            # éœ€è¦åœæ­¢éƒ¨åˆ† Worker
            for i in range(current_count - 1, target_count - 1, -1):
                worker_id, process = self.worker_processes[i]
                process.terminate()
                process.wait()
                self.worker_processes.pop()
                print(f"ğŸ›‘ Worker {worker_id} å·²åœæ­¢")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    manager = DynamicWorkerManager(
        master_host="192.168.1.100",
        master_port=5557,
        locust_file="distributed_demo.py"
    )
    
    # åˆå§‹å¯åŠ¨ 2 ä¸ª Worker
    manager.scale_workers(2)
    time.sleep(30)
    
    # æ‰©å±•åˆ° 4 ä¸ª Worker
    manager.scale_workers(4)
    time.sleep(30)
    
    # ç¼©å‡åˆ° 1 ä¸ª Worker
    manager.scale_workers(1)
```

### 3. åˆ†å¸ƒå¼æ•°æ®å…±äº«å’ŒåŒæ­¥

```python
# distributed_with_shared_data.py
from locust import HttpUser, task, between, events
import redis
import json
import threading

class SharedDataUser(HttpUser):
    wait_time = between(1, 5)
    host = "http://api.example.com"
    
    def on_start(self):
        """åˆå§‹åŒ– Redis è¿æ¥ç”¨äºæ•°æ®å…±äº«"""
        self.redis_client = redis.Redis(
            host='redis-host', 
            port=6379, 
            db=0, 
            decode_responses=True
        )
        
        # æ³¨å†Œå½“å‰ Worker
        worker_id = self.get_worker_id()
        self.redis_client.sadd("active_workers", worker_id)
    
    def get_worker_id(self):
        """è·å– Worker å”¯ä¸€æ ‡è¯†"""
        import socket
        return f"{socket.gethostname()}-{threading.current_thread().name}"
    
    @task
    def shared_counter_test(self):
        """ä½¿ç”¨å…±äº«è®¡æ•°å™¨çš„æµ‹è¯•"""
        # åŸå­é€’å¢è®¡æ•°å™¨
        counter = self.redis_client.incr("global_request_counter")
        
        # æ‰§è¡Œè¯·æ±‚
        response = self.client.get(f"/api/data?request_id={counter}")
        
        # è®°å½•æ¯ä¸ª Worker çš„è¯·æ±‚æ•°
        worker_id = self.get_worker_id()
        self.redis_client.hincrby("worker_requests", worker_id, 1)
    
    @task
    def distributed_coordination(self):
        """åˆ†å¸ƒå¼åè°ƒç¤ºä¾‹"""
        worker_id = self.get_worker_id()
        
        # åªæœ‰ç¬¬ä¸€ä¸ªè·å–é”çš„ Worker æ‰§è¡Œç‰¹å®šæ“ä½œ
        lock_acquired = self.redis_client.setnx("distributed_lock", worker_id)
        if lock_acquired:
            try:
                # æ‰§è¡Œéœ€è¦åè°ƒçš„æ“ä½œ
                self.client.post("/api/coordinated-action")
                print(f"Worker {worker_id} è·å¾—äº†åˆ†å¸ƒå¼é”")
            finally:
                # é‡Šæ”¾é”
                self.redis_client.delete("distributed_lock")
        else:
            # å…¶ä»– Worker æ‰§è¡Œæ™®é€šæ“ä½œ
            self.client.get("/api/normal-action")

# åˆ†å¸ƒå¼äº‹ä»¶å¤„ç†
@events.test_start.add_listener
def on_test_start(environment, **kwargs):
    """æµ‹è¯•å¼€å§‹æ—¶åˆå§‹åŒ–å…±äº«æ•°æ®"""
    if not environment.parsed_options.master:
        return
    
    # åªåœ¨ Master èŠ‚ç‚¹æ‰§è¡Œ
    redis_client = redis.Redis(host='redis-host', port=6379, db=0)
    redis_client.delete("global_request_counter", "active_workers", "worker_requests", "distributed_lock")
    print("ğŸ§¹ æµ‹è¯•å¼€å§‹ï¼Œæ¸…ç†å…±äº«æ•°æ®")

@events.test_stop.add_listener
def on_test_stop(environment, **kwargs):
    """æµ‹è¯•ç»“æŸæ—¶ç»Ÿè®¡åˆ†å¸ƒå¼ç»“æœ"""
    if not environment.parsed_options.master:
        return
    
    redis_client = redis.Redis(host='redis-host', port=6379, db=0)
    
    total_requests = redis_client.get("global_request_counter") or 0
    active_workers = redis_client.scard("active_workers")
    worker_stats = redis_client.hgetall("worker_requests")
    
    print(f"\nğŸ“Š åˆ†å¸ƒå¼æµ‹è¯•ç»Ÿè®¡:")
    print(f"   æ€»è¯·æ±‚æ•°: {total_requests}")
    print(f"   æ´»è·ƒ Worker æ•°: {active_workers}")
    print(f"   å„ Worker è¯·æ±‚åˆ†å¸ƒ: {worker_stats}")
```

### 4. ä½¿ç”¨ Pytest è‡ªåŠ¨åŒ–åˆ†å¸ƒå¼æµ‹è¯•

```python
# test_distributed_performance.py
import pytest
import subprocess
import time
import requests
import os

class DistributedLocustTest:
    def __init__(self):
        self.master_process = None
        self.worker_processes = []
        self.locust_file = "distributed_demo.py"
    
    def start_master(self):
        """å¯åŠ¨ Master èŠ‚ç‚¹"""
        cmd = [
            'locust', '-f', self.locust_file,
            '--master', 
            '--headless',
            '--master-bind-host', '127.0.0.1',
            '--master-bind-port', '5557',
            '--web-host', '127.0.0.1',
            '--web-port', '8089',
            '--expect-workers', '2',  # æœŸæœ›çš„ Worker æ•°é‡
            '-u', '100',  # æ€»ç”¨æˆ·æ•°
            '-r', '10',   # å­µåŒ–ç‡
            '-t', '1m'    # è¿è¡Œæ—¶é—´
        ]
        
        self.master_process = subprocess.Popen(
            cmd, 
            stdout=subprocess.PIPE, 
            stderr=subprocess.PIPE
        )
        print("ğŸš€ Master èŠ‚ç‚¹å¯åŠ¨ä¸­...")
        time.sleep(5)  # ç­‰å¾… Master å¯åŠ¨
    
    def start_worker(self, worker_id):
        """å¯åŠ¨ Worker èŠ‚ç‚¹"""
        cmd = [
            'locust', '-f', self.locust_file,
            '--worker',
            '--master-host', '127.0.0.1',
            '--master-port', '5557'
        ]
        
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        self.worker_processes.append(process)
        print(f"ğŸ‘· Worker {worker_id} å¯åŠ¨ä¸­...")
        time.sleep(2)
    
    def stop_all(self):
        """åœæ­¢æ‰€æœ‰è¿›ç¨‹"""
        for process in self.worker_processes:
            process.terminate()
            process.wait()
        
        if self.master_process:
            self.master_process.terminate()
            self.master_process.wait()

def test_distributed_performance():
    """åˆ†å¸ƒå¼æ€§èƒ½æµ‹è¯•"""
    test_runner = DistributedLocustTest()
    
    try:
        # å¯åŠ¨ Master
        test_runner.start_master()
        
        # å¯åŠ¨ 2 ä¸ª Worker
        test_runner.start_worker(1)
        test_runner.start_worker(2)
        
        # ç­‰å¾…æµ‹è¯•å®Œæˆ
        print("â³ æµ‹è¯•è¿è¡Œä¸­...")
        time.sleep(70)  # 1åˆ†é’Ÿæµ‹è¯• + é¢å¤–æ—¶é—´
        
        # æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
        for i, process in enumerate(test_runner.worker_processes):
            assert process.poll() is None, f"Worker {i+1} æ„å¤–é€€å‡º"
        
        # å¯ä»¥æ·»åŠ æ›´å¤šæ–­è¨€ï¼Œæ¯”å¦‚æ£€æŸ¥ API å“åº”ç­‰
        
        print("âœ… åˆ†å¸ƒå¼æµ‹è¯•å®Œæˆ")
        
    finally:
        # æ¸…ç†
        test_runner.stop_all()

if __name__ == "__main__":
    test_distributed_performance()
```

## âš™ï¸ åˆ†å¸ƒå¼è¿è¡Œæœ€ä½³å®è·µ

### 1. ç½‘ç»œé…ç½®å»ºè®®

```bash
# ç¡®ä¿é˜²ç«å¢™å¼€æ”¾ç›¸å…³ç«¯å£
# Master éœ€è¦å¼€æ”¾ï¼š8089 (Web UI), 5557 (Worker é€šä¿¡)

# Ubuntu ç¤ºä¾‹
sudo ufw allow 8089/tcp
sudo ufw allow 5557/tcp

# CentOS ç¤ºä¾‹
sudo firewall-cmd --permanent --add-port=8089/tcp
sudo firewall-cmd --permanent --add-port=5557/tcp
sudo firewall-cmd --reload
```

### 2. æ€§èƒ½ä¼˜åŒ–é…ç½®

```bash
# è°ƒæ•´ç³»ç»Ÿé™åˆ¶ï¼ˆLinuxï¼‰
echo "* soft nofile 65535" >> /etc/security/limits.conf
echo "* hard nofile 65535" >> /etc/security/limits.conf

# å¯¹äºå¤§é‡ Workerï¼Œè°ƒæ•´ Master å†…å­˜
locust -f locustfile.py --master --master-bind-host=0.0.0.0 --master-bind-port=5557 --expect-workers=10
```

### 3. ç›‘æ§åˆ†å¸ƒå¼é›†ç¾¤

```python
# cluster_monitor.py
import psutil
import requests
import time
from datetime import datetime

def monitor_cluster(master_host="localhost", master_port=8089):
    """ç›‘æ§åˆ†å¸ƒå¼é›†ç¾¤çŠ¶æ€"""
    while True:
        try:
            # è·å– Locust ç»Ÿè®¡ä¿¡æ¯
            stats_url = f"http://{master_host}:{master_port}/stats/requests"
            response = requests.get(stats_url)
            stats = response.json()
            
            print(f"\nğŸ“ˆ é›†ç¾¤çŠ¶æ€ - {datetime.now()}")
            print(f"   æ€»ç”¨æˆ·æ•°: {stats.get('user_count', 0)}")
            print(f"   æ€» RPS: {stats.get('total_rps', 0):.2f}")
            print(f"   å¤±è´¥ç‡: {stats.get('fail_ratio', 0)*100:.2f}%")
            
            # ç³»ç»Ÿèµ„æºç›‘æ§
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            print(f"   CPU ä½¿ç”¨ç‡: {cpu_percent}%")
            print(f"   å†…å­˜ä½¿ç”¨ç‡: {memory.percent}%")
            
        except Exception as e:
            print(f"ç›‘æ§é”™è¯¯: {e}")
        
        time.sleep(10)

if __name__ == "__main__":
    monitor_cluster()
```

## ğŸ¯ æ€»ç»“

Locust åˆ†å¸ƒå¼è¿è¡Œçš„å…³é”®è¦ç‚¹ï¼š

1. **æ¶æ„æ¸…æ™°**ï¼šMaster-Worker æ¨¡å¼ï¼ŒMaster è´Ÿè´£åè°ƒï¼ŒWorker è´Ÿè´£äº§ç”Ÿè´Ÿè½½
2. **çµæ´»æ‰©å±•**ï¼šå¯ä»¥åŠ¨æ€å¢åŠ  Worker èŠ‚ç‚¹æ¥æå‡å¹¶å‘èƒ½åŠ›
3. **æ•°æ®å…±äº«**ï¼šé€šè¿‡ Redis ç­‰å¤–éƒ¨å­˜å‚¨å®ç° Worker é—´çš„æ•°æ®åŒæ­¥
4. **è‡ªåŠ¨åŒ–é›†æˆ**ï¼šå¯ä»¥ä¸ Pytestã€Docker ç­‰å·¥å…·é›†æˆå®ç°è‡ªåŠ¨åŒ–æµ‹è¯•
5. **ç›‘æ§å®Œå–„**ï¼šæä¾›å®Œæ•´çš„é›†ç¾¤çŠ¶æ€ç›‘æ§èƒ½åŠ›

é€šè¿‡åˆç†é…ç½®åˆ†å¸ƒå¼é›†ç¾¤ï¼Œä½ å¯ä»¥è½»æ¾æ¨¡æ‹Ÿæ•°ä¸‡ç”šè‡³æ•°åä¸‡çš„å¹¶å‘ç”¨æˆ·ï¼Œæ»¡è¶³å¤§è§„æ¨¡æ€§èƒ½æµ‹è¯•çš„éœ€æ±‚ã€‚