# Locust æµ‹è¯•ç»“æœç»Ÿè®¡åŠŸèƒ½è¯¦è§£

Locust æä¾›äº†å¼ºå¤§è€Œçµæ´»çš„æµ‹è¯•ç»“æœç»Ÿè®¡åŠŸèƒ½ï¼Œæ”¯æŒå®æ—¶ç›‘æ§ã€è¯¦ç»†æŠ¥å‘Šç”Ÿæˆå’Œè‡ªå®šä¹‰æŒ‡æ ‡æ”¶é›†ã€‚ä¸‹é¢å…¨é¢ä»‹ç» Locust çš„ç»Ÿè®¡åŠŸèƒ½å’Œä½¿ç”¨æ–¹æ³•ã€‚

## ğŸ“Š å®æ—¶ç»Ÿè®¡ç›‘æ§

### 1. Web UI å®æ—¶ç»Ÿè®¡ç•Œé¢

Locust çš„ Web ç•Œé¢æä¾›ä¸°å¯Œçš„å®æ—¶ç»Ÿè®¡ä¿¡æ¯ï¼š

```python
# ç¤ºä¾‹æµ‹è¯•è„šæœ¬ç”¨äºå±•ç¤ºç»Ÿè®¡åŠŸèƒ½
from locust import HttpUser, task, between
import random

class StatsDemoUser(HttpUser):
    wait_time = between(1, 3)
    host = "https://httpbin.org"
    
    @task(3)
    def get_status(self):
        status_codes = [200, 404, 500]
        code = random.choice(status_codes)
        self.client.get(f"/status/{code}", name="/status/[code]")
    
    @task(2)
    def get_delay(self):
        delay = random.randint(1, 5)
        self.client.get(f"/delay/{delay}", name="/delay/[seconds]")
    
    @task(1)
    def post_data(self):
        self.client.post("/post", json={"test": "data"}, name="/post")
```

è®¿é—® `http://localhost:8089` å¯ä»¥çœ‹åˆ°ï¼š

- **Statistics æ ‡ç­¾é¡µ**ï¼šè¯·æ±‚çº§åˆ«çš„è¯¦ç»†ç»Ÿè®¡
- **Charts æ ‡ç­¾é¡µ**ï¼šå®æ—¶å›¾è¡¨å±•ç¤º
- **Failures æ ‡ç­¾é¡µ**ï¼šå¤±è´¥è¯·æ±‚è¯¦æƒ…
- **Exceptions æ ‡ç­¾é¡µ**ï¼šå¼‚å¸¸ä¿¡æ¯
- **Download Data**ï¼šä¸‹è½½æµ‹è¯•æ•°æ®

### 2. ç»Ÿè®¡æŒ‡æ ‡è¯´æ˜

**Statistics è¡¨æ ¼åŒ…å«ä»¥ä¸‹å…³é”®æŒ‡æ ‡ï¼š**

| æŒ‡æ ‡                   | è¯´æ˜                     |
| ---------------------- | ------------------------ |
| **Type**               | è¯·æ±‚ç±»å‹ (GET/POSTç­‰)    |
| **Name**               | è¯·æ±‚åç§°ï¼ˆæ”¯æŒåç§°åˆ†ç»„ï¼‰ |
| **# Requests**         | æ€»è¯·æ±‚æ•°                 |
| **# Fails**            | å¤±è´¥è¯·æ±‚æ•°               |
| **Median**             | å“åº”æ—¶é—´ä¸­ä½æ•°           |
| **90%ile**             | 90% è¯·æ±‚çš„å“åº”æ—¶é—´       |
| **95%ile**             | 95% è¯·æ±‚çš„å“åº”æ—¶é—´       |
| **99%ile**             | 99% è¯·æ±‚çš„å“åº”æ—¶é—´       |
| **Average**            | å¹³å‡å“åº”æ—¶é—´             |
| **Min**                | æœ€å°å“åº”æ—¶é—´             |
| **Max**                | æœ€å¤§å“åº”æ—¶é—´             |
| **Average Size**       | å¹³å‡å“åº”å¤§å°             |
| **Current RPS**        | å½“å‰æ¯ç§’è¯·æ±‚æ•°           |
| **Current Failures/s** | å½“å‰æ¯ç§’å¤±è´¥æ•°           |

## ğŸ’¾ ç»Ÿè®¡æŠ¥å‘Šå¯¼å‡º

### 1. CSV æŠ¥å‘Šç”Ÿæˆ

```bash
# ç”Ÿæˆ CSV æŠ¥å‘Šæ–‡ä»¶
locust -f stats_demo.py --headless --users 10 --spawn-rate 1 --run-time 30s --csv=test_results

# ç”Ÿæˆçš„æ–‡ä»¶ï¼š
# test_results_stats.csv      # ç»Ÿè®¡æ‘˜è¦
# test_results_stats_history.csv # å†å²ç»Ÿè®¡æ•°æ®
# test_results_failures.csv   # å¤±è´¥è®°å½•
# test_results_exceptions.csv # å¼‚å¸¸è®°å½•
```

**CSV æ–‡ä»¶å†…å®¹ç¤ºä¾‹ï¼š**
```csv
Type,Name,Request Count,Failure Count,Median Response Time,Average Response Time,Min Response Time,Max Response Time,Average Content Size,Requests/s,Failures/s,50%,66%,75%,80%,90%,95%,98%,99%,99.9%,99.99%,100%
GET,/status/[code],150,50,210,245,120,890,145,5.0,1.67,210,280,320,350,420,480,520,550,600,620,890
POST,/post,50,2,180,195,130,450,230,1.67,0.07,180,210,240,260,300,330,380,400,420,440,450
```

### 2. HTML æŠ¥å‘Š

```bash
# ç”Ÿæˆ HTML æŠ¥å‘Š
locust -f stats_demo.py --headless --users 100 --spawn-rate 10 --run-time 1m --html=report.html
```

### 3. JSON æ ¼å¼ç»Ÿè®¡

```bash
# ç”Ÿæˆ JSON æ ¼å¼çš„ç»Ÿè®¡æ•°æ®
locust -f stats_demo.py --headless --users 10 --run-time 10s --json --json-save=stats.json
```

## ğŸ”§ è‡ªå®šä¹‰ç»Ÿè®¡åŠŸèƒ½

### 1. è‡ªå®šä¹‰æŒ‡æ ‡æ”¶é›†

```python
from locust import HttpUser, task, between, events
import time
import statistics

class CustomMetricsCollector:
    def __init__(self):
        self.response_times = []
        self.slow_requests = 0
        self.custom_metrics = {}
        
    @events.request_success.add_listener
    def on_request_success(self, request_type, name, response_time, response_length, **kwargs):
        # æ”¶é›†å“åº”æ—¶é—´
        self.response_times.append(response_time)
        
        # ç»Ÿè®¡æ…¢è¯·æ±‚
        if response_time > 1000:
            self.slow_requests += 1
            
        # æŒ‰ç«¯ç‚¹åˆ†ç»„ç»Ÿè®¡
        if name not in self.custom_metrics:
            self.custom_metrics[name] = []
        self.custom_metrics[name].append(response_time)
    
    @events.test_stop.add_listener
    def on_test_stop(self, environment, **kwargs):
        """æµ‹è¯•ç»“æŸæ—¶ç”Ÿæˆè‡ªå®šä¹‰æŠ¥å‘Š"""
        if self.response_times:
            print(f"\nğŸ“Š è‡ªå®šä¹‰ç»Ÿè®¡æŠ¥å‘Š:")
            print(f"   æ€»è¯·æ±‚æ•°: {len(self.response_times)}")
            print(f"   å¹³å‡å“åº”æ—¶é—´: {statistics.mean(self.response_times):.2f}ms")
            print(f"   å“åº”æ—¶é—´æ ‡å‡†å·®: {statistics.stdev(self.response_times):.2f}ms")
            print(f"   æ…¢è¯·æ±‚æ•° (>1000ms): {self.slow_requests}")
            print(f"   æ…¢è¯·æ±‚æ¯”ä¾‹: {(self.slow_requests/len(self.response_times)*100):.2f}%")
            
            # å„ç«¯ç‚¹ç»Ÿè®¡
            for endpoint, times in self.custom_metrics.items():
                avg_time = statistics.mean(times)
                p95 = statistics.quantiles(times, n=100)[94]
                print(f"   {endpoint}: å¹³å‡{avg_time:.2f}ms, P95={p95:.2f}ms")

# åˆå§‹åŒ–è‡ªå®šä¹‰æŒ‡æ ‡æ”¶é›†å™¨
metrics = CustomMetricsCollector()
```

### 2. å“åº”æ—¶é—´åˆ†æ¡¶ç»Ÿè®¡

```python
from locust import events
from collections import defaultdict

class ResponseTimeBuckets:
    def __init__(self):
        self.buckets = defaultdict(int)
        self.bucket_ranges = [
            (0, 100), (100, 200), (200, 500), 
            (500, 1000), (1000, 2000), (2000, float('inf'))
        ]
    
    @events.request_success.add_listener
    def bucket_response_time(self, request_type, name, response_time, response_length, **kwargs):
        for min_time, max_time in self.bucket_ranges:
            if min_time <= response_time < max_time:
                self.buckets[(min_time, max_time)] += 1
                break
    
    @events.test_stop.add_listener
    def print_bucket_stats(self, environment, **kwargs):
        print("\nğŸ“ˆ å“åº”æ—¶é—´åˆ†å¸ƒ:")
        total_requests = sum(self.buckets.values())
        for bucket, count in sorted(self.buckets.items()):
            percentage = (count / total_requests * 100) if total_requests > 0 else 0
            print(f"   {bucket[0]}-{bucket[1]}ms: {count} è¯·æ±‚ ({percentage:.1f}%)")

buckets = ResponseTimeBuckets()
```

### 3. ä¸šåŠ¡æŒ‡æ ‡ç»Ÿè®¡

```python
class BusinessMetrics:
    def __init__(self):
        self.transactions = {
            'login_success': 0,
            'login_failure': 0,
            'purchase_success': 0,
            'purchase_failure': 0
        }
        self.revenue = 0
        
    def record_login(self, success=True):
        if success:
            self.transactions['login_success'] += 1
        else:
            self.transactions['login_failure'] += 1
    
    def record_purchase(self, amount, success=True):
        if success:
            self.transactions['purchase_success'] += 1
            self.revenue += amount
        else:
            self.transactions['purchase_failure'] += 1
    
    @events.test_stop.add_listener
    def print_business_report(self, environment, **kwargs):
        print("\nğŸ’° ä¸šåŠ¡æŒ‡æ ‡æŠ¥å‘Š:")
        total_logins = self.transactions['login_success'] + self.transactions['login_failure']
        login_success_rate = (self.transactions['login_success'] / total_logins * 100) if total_logins > 0 else 0
        
        total_purchases = self.transactions['purchase_success'] + self.transactions['purchase_failure']
        purchase_success_rate = (self.transactions['purchase_success'] / total_purchases * 100) if total_purchases > 0 else 0
        
        print(f"   ç™»å½•æˆåŠŸç‡: {login_success_rate:.1f}%")
        print(f"   è´­ä¹°æˆåŠŸç‡: {purchase_success_rate:.1f}%")
        print(f"   æ€»è¥æ”¶: ${self.revenue:.2f}")
        print(f"   å¹³å‡è®¢å•ä»·å€¼: ${self.revenue/self.transactions['purchase_success']:.2f}" if self.transactions['purchase_success'] > 0 else "   å¹³å‡è®¢å•ä»·å€¼: $0.00")

business_metrics = BusinessMetrics()

class EcommerceUser(HttpUser):
    wait_time = between(1, 3)
    host = "https://api.example.com"
    
    @task
    def login(self):
        response = self.client.post("/login", json={
            "username": "test", 
            "password": "password"
        })
        if response.status_code == 200:
            business_metrics.record_login(success=True)
        else:
            business_metrics.record_login(success=False)
    
    @task
    def purchase(self):
        order_amount = random.randint(10, 100)
        response = self.client.post("/purchase", json={
            "amount": order_amount,
            "product_id": random.randint(1, 10)
        })
        if response.status_code == 200:
            business_metrics.record_purchase(order_amount, success=True)
        else:
            business_metrics.record_purchase(order_amount, success=False)
```

## ğŸ“ˆ é«˜çº§ç»Ÿè®¡åŠŸèƒ½

### 1. å®æ—¶ç»Ÿè®¡æ•°æ®å¤„ç†

```python
import json
import requests
from locust import events
from threading import Thread
import time

class RealTimeStatsProcessor:
    def __init__(self, dashboard_url=None):
        self.dashboard_url = dashboard_url
        self.stats_buffer = []
        self.running = False
        
    def start_realtime_processing(self):
        """å¯åŠ¨å®æ—¶ç»Ÿè®¡å¤„ç†çº¿ç¨‹"""
        self.running = True
        self.process_thread = Thread(target=self._process_stats)
        self.process_thread.daemon = True
        self.process_thread.start()
    
    def _process_stats(self):
        """å®æ—¶å¤„ç†ç»Ÿè®¡æ•°æ®çš„åå°çº¿ç¨‹"""
        while self.running:
            if self.stats_buffer:
                stats = self.stats_buffer.pop(0)
                self._send_to_dashboard(stats)
                self._check_alerts(stats)
            time.sleep(5)  # æ¯5ç§’å¤„ç†ä¸€æ¬¡
    
    @events.request_success.add_listener
    @events.request_failure.add_listener
    def collect_realtime_stats(self, **kwargs):
        """æ”¶é›†å®æ—¶ç»Ÿè®¡æ•°æ®"""
        current_stats = {
            'timestamp': time.time(),
            'metrics': kwargs
        }
        self.stats_buffer.append(current_stats)
    
    def _send_to_dashboard(self, stats):
        """å‘é€ç»Ÿè®¡æ•°æ®åˆ°ç›‘æ§é¢æ¿"""
        if self.dashboard_url:
            try:
                requests.post(self.dashboard_url, 
                            json=stats, 
                            timeout=2)
            except Exception as e:
                print(f"å‘é€åˆ°ç›‘æ§é¢æ¿å¤±è´¥: {e}")
    
    def _check_alerts(self, stats):
        """æ£€æŸ¥ç»Ÿè®¡è­¦æŠ¥"""
        response_time = stats['metrics'].get('response_time', 0)
        if response_time > 5000:  # 5ç§’é˜ˆå€¼
            self._trigger_alert(f"é«˜å“åº”æ—¶é—´è­¦æŠ¥: {response_time}ms")
    
    def _trigger_alert(self, message):
        """è§¦å‘è­¦æŠ¥"""
        print(f"ğŸš¨ {message}")
        # å¯ä»¥é›†æˆåˆ° Slackã€é‚®ä»¶ã€çŸ­ä¿¡ç­‰é€šçŸ¥ç³»ç»Ÿ

# ä½¿ç”¨å®æ—¶ç»Ÿè®¡å¤„ç†å™¨
realtime_processor = RealTimeStatsProcessor()
realtime_processor.start_realtime_processing()
```

### 2. åˆ†å¸ƒå¼ç»Ÿè®¡èšåˆ

```python
from locust import events
import redis
import json

class DistributedStatsAggregator:
    def __init__(self, redis_host='localhost', redis_port=6379):
        self.redis = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)
        self.stats_key = "locust:stats"
        
    @events.init.add_listener
    def setup_distributed_stats(self, environment, **kwargs):
        """è®¾ç½®åˆ†å¸ƒå¼ç»Ÿè®¡"""
        if environment.parsed_options.master:
            # Master èŠ‚ç‚¹åˆå§‹åŒ–ç»Ÿè®¡
            self.redis.delete(self.stats_key)
        elif environment.parsed_options.worker:
            # Worker èŠ‚ç‚¹è®¾ç½®ç»Ÿè®¡æŠ¥å‘Š
            environment.events.report_to_master.add_listener(self.on_worker_report)
    
    def on_worker_report(self, client_id, data):
        """Worker å‘ Master æŠ¥å‘Šè‡ªå®šä¹‰ç»Ÿè®¡"""
        custom_stats = {
            'worker_id': client_id,
            'timestamp': time.time(),
            'custom_metrics': {
                'active_users': len(self.get_active_users()),
                'cache_hit_rate': self.calculate_cache_hit_rate()
            }
        }
        data['custom_stats'] = custom_stats
    
    @events.worker_report.add_listener
    def on_worker_report_receive(self, client_id, data):
        """Master æ¥æ”¶ Worker æŠ¥å‘Š"""
        if 'custom_stats' in data:
            # å­˜å‚¨ Worker ç»Ÿè®¡
            worker_stats = data['custom_stats']
            self.redis.hset(self.stats_key, client_id, json.dumps(worker_stats))
            
            # èšåˆæ‰€æœ‰ Worker ç»Ÿè®¡
            self.aggregate_worker_stats()
    
    def aggregate_worker_stats(self):
        """èšåˆæ‰€æœ‰ Worker çš„ç»Ÿè®¡"""
        all_stats = self.redis.hgetall(self.stats_key)
        aggregated = {
            'total_workers': len(all_stats),
            'aggregated_metrics': {},
            'timestamp': time.time()
        }
        
        for worker_id, stats_json in all_stats.items():
            stats = json.loads(stats_json)
            for metric, value in stats.get('custom_metrics', {}).items():
                if metric not in aggregated['aggregated_metrics']:
                    aggregated['aggregated_metrics'][metric] = []
                aggregated['aggregated_metrics'][metric].append(value)
        
        # è®¡ç®—èšåˆå€¼ï¼ˆå¹³å‡å€¼ï¼‰
        for metric, values in aggregated['aggregated_metrics'].items():
            aggregated['aggregated_metrics'][f"{metric}_avg"] = sum(values) / len(values)
        
        print(f"ğŸ“Š åˆ†å¸ƒå¼ç»Ÿè®¡èšåˆ: {aggregated}")

# åˆå§‹åŒ–åˆ†å¸ƒå¼ç»Ÿè®¡èšåˆå™¨
distributed_stats = DistributedStatsAggregator()
```

### 3. ç»Ÿè®¡æ•°æ®åˆ†æä¸å¯è§†åŒ–

```python
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime

class StatsAnalyzer:
    def __init__(self):
        self.historical_data = []
    
    def load_stats_from_csv(self, csv_file):
        """ä» CSV æ–‡ä»¶åŠ è½½ç»Ÿè®¡æ•°æ®"""
        df = pd.read_csv(csv_file)
        return df
    
    def analyze_performance_trends(self, stats_df):
        """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
        print("\nğŸ“ˆ æ€§èƒ½è¶‹åŠ¿åˆ†æ:")
        
        # è®¡ç®—å…³é”®æŒ‡æ ‡
        total_requests = stats_df['Request Count'].sum()
        total_failures = stats_df['Failure Count'].sum()
        failure_rate = (total_failures / total_requests * 100) if total_requests > 0 else 0
        
        avg_response_time = stats_df['Average Response Time'].mean()
        p95_response_time = stats_df['95%ile'].mean()
        
        print(f"   æ€»è¯·æ±‚æ•°: {total_requests}")
        print(f"   å¤±è´¥ç‡: {failure_rate:.2f}%")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}ms")
        print(f"   P95å“åº”æ—¶é—´: {p95_response_time:.2f}ms")
        
        # è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
        slow_endpoints = stats_df[stats_df['Average Response Time'] > 1000]
        if not slow_endpoints.empty:
            print("\nâš ï¸  æ…¢ç«¯ç‚¹è¯†åˆ«:")
            for _, endpoint in slow_endpoints.iterrows():
                print(f"   {endpoint['Name']}: {endpoint['Average Response Time']:.2f}ms")
    
    def create_performance_report(self, stats_files):
        """åˆ›å»ºæ€§èƒ½æŠ¥å‘Š"""
        plt.style.use('seaborn-v0_8')
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Locust æ€§èƒ½æµ‹è¯•æŠ¥å‘Š', fontsize=16)
        
        # åŠ è½½å¹¶åˆ†ææ¯ä¸ªæµ‹è¯•çš„æ•°æ®
        for i, stats_file in enumerate(stats_files):
            df = self.load_stats_from_csv(stats_file)
            test_name = f"Test {i+1}"
            
            # å“åº”æ—¶é—´åˆ†å¸ƒ
            axes[0, 0].bar([f"{test_name}\n{row['Name'][:15]}" for _, row in df.iterrows()], 
                          df['Average Response Time'], label=test_name, alpha=0.7)
            axes[0, 0].set_title('å¹³å‡å“åº”æ—¶é—´')
            axes[0, 0].tick_params(axis='x', rotation=45)
            
            # å¤±è´¥ç‡
            failure_rates = (df['Failure Count'] / df['Request Count'] * 100).fillna(0)
            axes[0, 1].bar([f"{test_name}\n{row['Name'][:15]}" for _, row in df.iterrows()], 
                          failure_rates, label=test_name, alpha=0.7)
            axes[0, 1].set_title('å¤±è´¥ç‡ (%)')
            axes[0, 1].tick_params(axis='x', rotation=45)
            
            # è¯·æ±‚é‡åˆ†å¸ƒ
            axes[1, 0].pie(df['Request Count'], labels=df['Name'], autopct='%1.1f%%')
            axes[1, 0].set_title('è¯·æ±‚é‡åˆ†å¸ƒ')
            
            # å“åº”æ—¶é—´ç™¾åˆ†ä½
            percentiles = ['50%', '90%', '95%', '99%']
            p_values = [df['Median Response Time'].mean(), 
                       df['90%ile'].mean(), 
                       df['95%ile'].mean(), 
                       df['99%ile'].mean()]
            axes[1, 1].bar(percentiles, p_values)
            axes[1, 1].set_title('å“åº”æ—¶é—´ç™¾åˆ†ä½')
        
        plt.tight_layout()
        plt.savefig('performance_report.png', dpi=300, bbox_inches='tight')
        plt.show()

# ä½¿ç”¨ç»Ÿè®¡åˆ†æå™¨
analyzer = StatsAnalyzer()

@events.test_stop.add_listener
def on_test_stop_analysis(environment, **kwargs):
    """æµ‹è¯•ç»“æŸæ—¶è¿›è¡Œç»Ÿè®¡åˆ†æ"""
    # è¿™é‡Œå¯ä»¥è‡ªåŠ¨åŠ è½½æœ€æ–°ç”Ÿæˆçš„ CSV æ–‡ä»¶è¿›è¡Œåˆ†æ
    csv_files = ["test_results_stats.csv"]  # å®é™…ä½¿ç”¨æ—¶åŠ¨æ€è·å–
    for csv_file in csv_files:
        try:
            df = analyzer.load_stats_from_csv(csv_file)
            analyzer.analyze_performance_trends(df)
        except FileNotFoundError:
            print(f"ç»Ÿè®¡æ–‡ä»¶ {csv_file} æœªæ‰¾åˆ°")
```

## âš™ï¸ ç»Ÿè®¡é…ç½®é€‰é¡¹

### 1. å‘½ä»¤è¡Œç»Ÿè®¡é€‰é¡¹

```bash
# å®Œæ•´çš„ç»Ÿè®¡ç›¸å…³å‘½ä»¤è¡Œé€‰é¡¹
locust -f locustfile.py \
  --headless \
  --users 100 \
  --spawn-rate 10 \
  --run-time 5m \
  --csv=results \           # CSV æ–‡ä»¶å‰ç¼€
  --csv-full-history \      # ä¿å­˜å®Œæ•´å†å²ç»Ÿè®¡
  --html=report.html \      # HTML æŠ¥å‘Š
  --json \                  # å¯ç”¨ JSON è¾“å‡º
  --json-save=stats.json \  # ä¿å­˜ JSON ç»Ÿè®¡
  --print-stats \           # æ§åˆ¶å°æ‰“å°ç»Ÿè®¡
  --only-summary \          # åªæ˜¾ç¤ºæ‘˜è¦
  --reset-stats \           # é‡ç½®ç»Ÿè®¡ï¼ˆåˆ†å¸ƒå¼æ¨¡å¼ï¼‰
  --expect-workers=4        # æœŸæœ›çš„ Worker æ•°é‡
```

### 2. ç»Ÿè®¡é‡ç½®å’Œæ§åˆ¶

```python
from locust import events
from locust.runners import MasterRunner, WorkerRunner

class StatsController:
    def __init__(self):
        self.test_phases = []
        self.current_phase = 0
    
    @events.test_start.add_listener
    def on_test_start(self, environment, **kwargs):
        """æµ‹è¯•å¼€å§‹æ—¶é‡ç½®ç»Ÿè®¡"""
        if isinstance(environment.runner, MasterRunner):
            print("ğŸ§¹ é‡ç½®ç»Ÿè®¡ä¿¡æ¯...")
            # Master èŠ‚ç‚¹å¯ä»¥æ§åˆ¶ç»Ÿè®¡é‡ç½®
            environment.runner.stats.clear_all()
    
    def start_new_phase(self, phase_name, users, spawn_rate):
        """å¼€å§‹æ–°çš„æµ‹è¯•é˜¶æ®µ"""
        self.current_phase += 1
        self.test_phases.append({
            'name': phase_name,
            'users': users,
            'spawn_rate': spawn_rate,
            'start_time': time.time()
        })
        print(f"ğŸ” å¼€å§‹æµ‹è¯•é˜¶æ®µ: {phase_name}")

stats_controller = StatsController()
```

## ğŸ¯ ç»Ÿè®¡åŠŸèƒ½æœ€ä½³å®è·µ

### 1. ç”Ÿäº§ç¯å¢ƒç»Ÿè®¡é…ç½®

```python
# production_stats.py
from locust import HttpUser, task, between, events
import logging
import sys

class ProductionStatsConfig:
    def __init__(self):
        self.setup_logging()
        self.setup_stats_handlers()
    
    def setup_logging(self):
        """é…ç½®ç»Ÿè®¡æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('locust_stats.log'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger('locust_stats')
    
    def setup_stats_handlers(self):
        """è®¾ç½®ç»Ÿè®¡å¤„ç†å™¨"""
        @events.request_success.add_listener
        def log_success(request_type, name, response_time, response_length, **kwargs):
            if response_time > 1000:  # åªè®°å½•æ…¢è¯·æ±‚
                self.logger.info(f"æ…¢è¯·æ±‚: {name} - {response_time}ms")
        
        @events.request_failure.add_listener  
        def log_failure(request_type, name, response_time, exception, **kwargs):
            self.logger.warning(f"è¯·æ±‚å¤±è´¥: {name} - {exception}")
        
        @events.test_stop.add_listener
        def final_stats_report(environment, **kwargs):
            stats = environment.stats
            total = stats.total
            
            report = f"""
ğŸ¯ æµ‹è¯•å®ŒæˆæŠ¥å‘Š:
   æ€»è¯·æ±‚æ•°: {total.num_requests}
   å¤±è´¥è¯·æ±‚: {total.num_failures}
   å¤±è´¥ç‡: {(total.num_failures/total.num_requests*100):.2f}%
   å¹³å‡å“åº”æ—¶é—´: {total.avg_response_time:.2f}ms
   æœ€å¤§å“åº”æ—¶é—´: {total.max_response_time}ms
   æ€»RPS: {total.total_rps:.2f}
            """
            self.logger.info(report)

# åº”ç”¨ç”Ÿäº§ç¯å¢ƒç»Ÿè®¡é…ç½®
prod_stats = ProductionStatsConfig()
```

### 2. æ€§èƒ½é˜ˆå€¼ç›‘æ§

```python
class PerformanceThresholds:
    def __init__(self):
        self.thresholds = {
            'max_avg_response_time': 500,     # å¹³å‡å“åº”æ—¶é—´ä¸Šé™
            'max_p95_response_time': 1000,    # P95å“åº”æ—¶é—´ä¸Šé™  
            'max_failure_rate': 1.0,          # å¤±è´¥ç‡ä¸Šé™ (%)
            'min_rps': 50                     # æœ€ä½ RPS
        }
        self.violations = []
    
    @events.request.add_listener
    def check_thresholds(self, request_type, name, response_time, response_length, exception, **kwargs):
        """æ£€æŸ¥æ€§èƒ½é˜ˆå€¼"""
        # è¿™é‡Œå¯ä»¥å®æ—¶ç›‘æ§é˜ˆå€¼
        pass
    
    @events.test_stop.add_listener
    def final_threshold_check(self, environment, **kwargs):
        """æœ€ç»ˆé˜ˆå€¼æ£€æŸ¥"""
        stats = environment.stats.total
        
        # æ£€æŸ¥å„é¡¹é˜ˆå€¼
        if stats.avg_response_time > self.thresholds['max_avg_response_time']:
            self.violations.append(f"å¹³å‡å“åº”æ—¶é—´ {stats.avg_response_time:.2f}ms è¶…è¿‡é˜ˆå€¼ {self.thresholds['max_avg_response_time']}ms")
        
        failure_rate = (stats.num_failures / stats.num_requests * 100) if stats.num_requests > 0 else 0
        if failure_rate > self.thresholds['max_failure_rate']:
            self.violations.append(f"å¤±è´¥ç‡ {failure_rate:.2f}% è¶…è¿‡é˜ˆå€¼ {self.thresholds['max_failure_rate']}%")
        
        if stats.total_rps < self.thresholds['min_rps']:
            self.violations.append(f"RPS {stats.total_rps:.2f} ä½äºé˜ˆå€¼ {self.thresholds['min_rps']}")
        
        # è¾“å‡ºé˜ˆå€¼æ£€æŸ¥ç»“æœ
        if self.violations:
            print("âŒ æ€§èƒ½é˜ˆå€¼è¿å:")
            for violation in self.violations:
                print(f"   - {violation}")
            # å¯ä»¥åœ¨è¿™é‡Œè§¦å‘è­¦æŠ¥æˆ–ä½¿æµ‹è¯•å¤±è´¥
        else:
            print("âœ… æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡ç¬¦åˆè¦æ±‚")

threshold_monitor = PerformanceThresholds()
```

## ğŸ“‹ æ€»ç»“

Locust çš„ç»Ÿè®¡åŠŸèƒ½æä¾›äº†ï¼š

1. **å®æ—¶ç›‘æ§**ï¼šWeb UI å®æ—¶å±•ç¤ºå…³é”®æŒ‡æ ‡
2. **å¤šç§æŠ¥å‘Šæ ¼å¼**ï¼šCSVã€HTMLã€JSON ç­‰æ ¼å¼çš„æŠ¥å‘Š
3. **è‡ªå®šä¹‰æŒ‡æ ‡**ï¼šçµæ´»æ”¶é›†ä¸šåŠ¡ç›¸å…³æŒ‡æ ‡
4. **åˆ†å¸ƒå¼ç»Ÿè®¡**ï¼šæ”¯æŒå¤šèŠ‚ç‚¹ç»Ÿè®¡èšåˆ
5. **æ•°æ®åˆ†æ**ï¼šå¼ºå¤§çš„æ•°æ®åˆ†æå’Œå¯è§†åŒ–èƒ½åŠ›
6. **é˜ˆå€¼ç›‘æ§**ï¼šæ€§èƒ½æŒ‡æ ‡é˜ˆå€¼æ£€æŸ¥å’Œè­¦æŠ¥

é€šè¿‡è¿™äº›åŠŸèƒ½ï¼Œä½ å¯ä»¥å…¨é¢æŒæ¡ç³»ç»Ÿæ€§èƒ½è¡¨ç°ï¼Œå¿«é€Ÿå®šä½æ€§èƒ½ç“¶é¢ˆï¼Œå¹¶ç”Ÿæˆä¸“ä¸šçš„æµ‹è¯•æŠ¥å‘Šã€‚åˆç†åˆ©ç”¨ Locust çš„ç»Ÿè®¡åŠŸèƒ½ï¼Œå¯ä»¥å¤§å¤§æé«˜æ€§èƒ½æµ‹è¯•çš„æ•ˆç‡å’Œè´¨é‡ã€‚