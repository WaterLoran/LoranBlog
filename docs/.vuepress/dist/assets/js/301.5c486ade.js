(window.webpackJsonp=window.webpackJsonp||[]).push([[301],{731:function(v,_,t){"use strict";t.r(_);var a=t(2),r=Object(a.a)({},(function(){var v=this,_=v._self._c;return _("ContentSlotsDistributor",{attrs:{"slot-key":v.$parent.slotKey}},[_("h1",{attrs:{id:"自动化测试落地方案及要求"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#自动化测试落地方案及要求"}},[v._v("#")]),v._v(" 自动化测试落地方案及要求")]),v._v(" "),_("p",[v._v("本人以作者的实际经验总结而得, 即自动化从0-1过程中, 所遇到的问题的一些归纳总结.")]),v._v(" "),_("h2",{attrs:{id:"人员"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#人员"}},[v._v("#")]),v._v(" 人员")]),v._v(" "),_("ol",[_("li",[v._v("期望原部门领导能理解自动化的作用和目的, 能够明确他的投入以及产出预期\n自动化的落地是一项投入大, 但产出可能没有那么大的工作内容. 通常情况下, 功能测试在回归测试阶段能够发现90-99%的BUG, 但是仍然有可能会因为记忆, 工作时间等因素将少量BUG遗漏出去, 自动化测试本质是一个回归测试, 他负责使用已有的自动化脚本做尽可能全面的回归测试, 发现的BUG通常是改动引发的, 而不能像功能测试中的探索测试一样去发现一些未知的新BUG. 或者一些特殊的改动引发.\n所以, 主导者或者领导者不能期望他去发现很多BUG 也不能期望他发现的BUG, 有功能测试那么做, 甚至不能期望他发现的BUG, 有功能测试发现的1/20-1/00. 如果有这么多, 应该及时反省功能测试质量, 以及编码编写质量, 因为这个过程实在是引入了过多的改动引发")]),v._v(" "),_("li",[v._v("期望该部门中的自动化人力投入是充足的, 正常情况下自动化的人力应该是和自动化一致的\n根据笔者的经验, 第一家公司功能测试人员比自动化测试人员大概是2:1, 第二家大概是2:1, 在架构不稳定, 业务不稳定的初期是有可能达到1:1的. 所以主导者或者领导要能够投入充足的人力, 当然不一定都要熟悉自动化的人员, 通常而言只要有技术主导者, 其他人员全部配成实习生或者校招生也是可行的.")]),v._v(" "),_("li",[v._v("期望部门中做自动化的人员, 是有技术梯度层级的\n这样子设计, 主要是为了能够保证有一个人能够主导自动化测试体系的建设, 并且他能够把一些重复低效的工作内容分给新手(比如实习生或者 校招生), 从而保证他有足够的时间去规划设计为自动化框架或平台. 另外一个有梯度的组织, 能够较好的形成一种学习的氛围, 一些规范和要求也能够快速实施下去, 如果是技术能力较为相同的中级人员, 则有可能存在一定程度的竞争, 如果是两个技术很完善的高级测试人员, 则有可能造成人员浪费")]),v._v(" "),_("li",[v._v("期望自动化工作是有中心化的管理人员的\n关于这点, 我听过太多故事, 就是在中小公司中, 自动化是极度难推行实施下去的, 场景的情况是, 那个领导既想要那个手下的人员完成功能测试, 也还能把自动化框架搭建起来, 并且在做的时候也是有点畏手畏脚的, 下不定决心要投入多少资源, 也找不到合适的人员来做, 最终的情况就是, 让手下的那个人搭建框架, 和制定规范. 但是问题处在于, 领导手下的那个人是没有什么实权的, 通常情况下, 沟通能力也不是很优秀的. 另外, 自动化在中小型公司通常是额外的工作量, 这更加增加了原有测试人员对这项工作内容的抗拒性, 所以这个在搞框架的人员, 通常是框架搞得不完善, 未能迭代到稳定的程度, 也很难去强有力的推行一些脚本规范, 也很难去向前要求功能测试的用例.\n所以, 这项工作内容必须是一位懂技术的领导全力推动, 或者是给足所有的权力和人力资源.")])]),v._v(" "),_("h2",{attrs:{id:"技术"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#技术"}},[v._v("#")]),v._v(" 技术")]),v._v(" "),_("ol",[_("li",[_("p",[v._v("部门要有能够独立设计编写落地自动化框架或者平台的人员, 一线城市至少25K")]),v._v(" "),_("p",[v._v("我看过市面上github上开源出来的框架, 几乎都是不能落地的, 参照于我说经历的两家已经成功落地自动化多年的大公司. 我可以有信心的说, 大部分框架还很幼稚, 或者不完善, 要么脚本编写难度大, 要么代码重复率高, 要么可读性不好. 但是世界是个巨大的草台班子, 在我所接触的人中, 能够独立去分析自动化框架并有代码能力去实现的, 薪资几乎都在25K左右.")])]),v._v(" "),_("li",[_("p",[v._v("期望要能够设计完善的脚本规范")]),v._v(" "),_("p",[v._v("其实这个脚本规范是强关联于自动化框架或者平台的, 通常情况下是设计实现自动化框架或平台的人同步制定的, 只要要明确的说明数据层的编写规范(或者说是抽象规范), 以及逻辑层或者(业务层的规范). 制定这些规范是为了脚本能够持续维护, 以及在组织之间长效流转. 以及脚本编写者能够快速去编写, 以及能够让配套工具具备可开发的可能性. 以及具备评审性, 以及能够支持框架的进一步迭代和新功能引入")])]),v._v(" "),_("li",[_("p",[v._v("期望功能测试输入到自动化测试的功能用例要可执行完善完整, 无歧义, 清晰明了")]),v._v(" "),_("p",[v._v("因为自动化测试常常是做集成级别的测试, 而非某个函数的单元测试(这种的测试, 一般是由开发去编写). 而自动化脚本编写的输入件是功能用例, 但是居然拿还有很多测试leader, 手下有5个人干活的那种, 都不能明白这种关系, 属实是学习或者培训不到位. 所以, 这里期望功能测试人员输入给自动化测试人员的功能用例是严谨的, 可执行的, 没有歧义的, 能够明确知道测试点的. 并且用例是全面的. 这样子自动化就只需要面向功能用例去编写脚本, 难度会大大降低, 以及协作的可能性会好很多. 如若不然, 自动化测试人员甚至都不知道, 具体要测试什么东西, 自然不知道测试什么, 检查什么, 构造什么样的数据. 我在第一第二家公司的时候, 功能用例都能写的很好, 很清晰, 可执行性很高, 测试点也明确.")])]),v._v(" "),_("li",[_("p",[v._v("期望有配套的工具和CICD工具平台")]),v._v(" "),_("p",[v._v("这里主要是能够减少编写脚本过程和实际的脚本生命周期中, 能够去减少很多时间. 比如可以有工具将功能用例转成自动化脚本的模板, 比如可以有工具, 自动生成关键字的信息, 也可以有CICD平台去定时连跑并收集各类信息, 最好是h有一个平台能够将各类信息收集起来, 并显示和查询, 期望可以有日志查看平台, 期望可以有快速修改维护自动化脚本的工具等等, 即自动化测试过程中, 也是会存在很大程度的重复工作, 而几乎这些工作都是可以开发对应的工具来减少工作量的.")])]),v._v(" "),_("li",[_("p",[v._v("期望功能测试质量是较好的")]),v._v(" "),_("p",[v._v("这里主要考虑一个效能问题, 和价值体现的问题, 如果说功能测试都做得很烂, 线上问题很多, 那么工作重点应该在功能测试的流程优化上, 功能测试的标准约束上, 以及功能测试人员的能力提升上, 而不是自动化工作内容上, 所以, 小公司的话, 自动化测试常常就变成了, 给领导们画饼的工具, 以及测试新手搞编码搞技术的一个狂欢, 实际上并不能做到多少效用. 比如线上问题每天反馈回来5个, 并且都是漏测的, 功能测试人员和开发处理不过来, 那么就算自动化每周发现2个BUG, 也是不够看的, 并且投入产出一对比, 就会发现还不如直接调转所有人去做功能呢. 不过嘛, 没经历过大公司的测试leader, 或者没有深入思考交流的人, 都会以为自动化的成效很高. 但实则这东西, 投入极大, 产出极小")])])]),v._v(" "),_("h2",{attrs:{id:"业务"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#业务"}},[v._v("#")]),v._v(" 业务")]),v._v(" "),_("ol",[_("li",[v._v("期望业务尽可能是基于web的, 或人员与软件的交互有严谨完善的协议\n这里主要是期望, 能够进行封装, 框架代码能够去尽量复用, 框架也能够尽可能的分层. 不然的话, 就会沦落为各类python脚本的堆砌, 即各类面向过程的代码的重复堆砌, 工程一旦大一点之后, 比如有100个脚本需要修改的时候, 就会发现修改起来极其麻烦, 就会想着直接就放弃这个自动化算了, 这也是常常自动化发展不起来的原因. 另外有这些晚上的协议, 一定程度上也是能够说明这个软件的可测试性和可交互性是具备的, 不然编写自动化的成本极大, 约等于每增加一个开发, 那就要增加一个自动化测试, 并且还得是编码经验丰富的测试人员, 投入就会非常大.")]),v._v(" "),_("li",[v._v("期望业务是稳定迭代的\n这里主要是考虑, 业务需求变更不大才有自动化测试的价值, 如果变化太大, 那么自动化所沉淀下来的关键字用处不大, 以及自动化脚本在编写之后, 很快又废弃掉, 就编写好之后没有连跑几次, 就废弃了, 这个成本是极大的, 预计要后面用于回归测试5次以上, 才会回本, 个人预期要用于10次回归才算是回本并且是有成效的.\n所以期望业务相对稳定的情况下采取做这些自动化测试, 并且还期望业务的变更会大概是在10%-30%之间的, 如果过小则可以用手工测试来做回归, 或者阅读代码来回归, 如果过大那可能说明该软件还在初始的迭代阶段, 不适合做自动化")]),v._v(" "),_("li",[v._v("期望业务具备一定程度的可测试性\n这点和第一点是挺像的, 挡在这里, 主要说明的是, 软件中, 各类数据易于构造, 易于查询, 而不太强依赖于被测系统或者第三方系统的数据, 这样子的情况是成本极高的, 比如云可视化工具就不适合做自动化, 因为云可视化他的数据很难构造, 并且构造的时候, 需要很多开发的基础知识, 需要很多网络知识, 这对普通的自动化测试人员来说, 是天书. 编码难度也是直线上升, 所投入的和产出的, 极度不平衡.  可能也不能满足预期.\n但是如果数据虽然是依赖于第三方, 但是是满足某项标准协议的, 那么就具备可测试性了, 比如核心网的各类网元的数据交互和传输, 他们都是满足通信协议的, 在这个时候, 其实就可以用软件工具来模拟这类信息的收发.")])])])}),[],!1,null,null,null);_.default=r.exports}}]);